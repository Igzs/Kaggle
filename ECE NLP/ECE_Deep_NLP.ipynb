{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge you will be building a model that automatically determines logical entailment between two sentences.  \n",
    "The model for this task we chose is a Bidirectionnial LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import train and test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Data/dataset_train.csv\",sep='\\t', index_col='index')\n",
    "test_df = pd.read_csv(\"Data/dataset_test_no_labels.csv\",sep='\\t',index_col='index')\n",
    "labels = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_1    0\n",
      "sentence_2    0\n",
      "label         0\n",
      "dtype: int64\n",
      "sentence_1    0\n",
      "sentence_2    0\n",
      "dtype: int64\n",
      "index\n",
      "0               neutral\n",
      "1            entailment\n",
      "2            entailment\n",
      "3            entailment\n",
      "4               neutral\n",
      "              ...      \n",
      "392657    contradiction\n",
      "392658          neutral\n",
      "392659       entailment\n",
      "392660          neutral\n",
      "392661          neutral\n",
      "Name: label, Length: 392662, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isna().sum())\n",
    "print(test_df.isna().sum())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       "       'Product and geography are what make cream skimming work. ',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conceptually cream skimming has two basic dimensions - product and geography.'\n",
      " 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him'\n",
      " 'One of our number will carry out your instructions minutely.' ...\n",
      " 'Houseboats are a beautifully preserved tradition of the heyday of the British Raj.'\n",
      " 'Obituaries fondly recalled his on-air debates and two thumbs up salutes with fellow reviewer Roger Ebert on their eponymous syndicated TV show.'\n",
      " 'in that other you know uh that i should do it or that or just to think about doing it rat her than having someone  tell him to do it i know that was a big thing in our house for a long time was that if i wanted my husband to do something to help']\n"
     ]
    }
   ],
   "source": [
    "print(train_df['sentence_1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 100\n",
    "max_vocabulary_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = train_df['sentence_1'].values\n",
    "sentence_1_words = [line.split() for line in sentence_1]\n",
    "assert( len(sentence_1_words) == len(sentence_1))\n",
    "\n",
    "sentence_2 = train_df['sentence_2'].values\n",
    "sentence_2_words = [line.split() for line in sentence_2]\n",
    "assert( len(sentence_2_words) == len(sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sentence_1_words = [item for sublist in sentence_1_words for item in sublist]\n",
    "flat_sentence_2_words = [item for sublist in sentence_2_words for item in sublist]\n",
    "flat_words = flat_sentence_1_words + flat_sentence_2_words\n",
    "assert(len(flat_words) == len(flat_sentence_1_words)+len(flat_sentence_2_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(set(flat_words))\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "word_index_inversed = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_vocabulary(corpus):\n",
    "    counter = Counter(corpus)\n",
    "    allowed_words = set([item[0] for item in counter.most_common(max_vocabulary_size)])\n",
    "    return [word for word in corpus if word in allowed_words]\n",
    "words = limit_vocabulary(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the input sentences to integer sequences using tf tokenizer.  \n",
    "We also need to pad the sequences to have them all at the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_vocabulary_size, char_level=False)\n",
    "tokenizer.fit_on_texts(words)\n",
    "sentence_1 =tokenizer.texts_to_sequences(train_df[\"sentence_1\"])\n",
    "sentence_2 = tokenizer.texts_to_sequences(train_df[\"sentence_2\"])\n",
    "sentence_1_seq = sequence.pad_sequences(sentence_1, maxlen=max_sequence_length, value=0,truncating=\"post\",padding=\"post\")\n",
    "sentence_2_seq  = sequence.pad_sequences( sentence_2,maxlen=max_sequence_length, value=0,truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "tmp.append(sentence_1_seq)\n",
    "tmp.append(sentence_2_seq)\n",
    "X = np.array(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_labels(labels,inverse=False):\n",
    "    convert_dict = {\n",
    "      'entailment': 0,\n",
    "      'neutral': 1,\n",
    "      'contradiction': 2\n",
    "    }\n",
    "    convert_dict_inverse = {\n",
    "      0: 'entailment',\n",
    "      1: 'neutral',\n",
    "      2: 'contradiction'\n",
    "    }\n",
    "    new_labels=[]\n",
    "    \n",
    "    for label in labels:\n",
    "        new_labels.append(convert_dict[label])\n",
    "    if inverse:\n",
    "        for label in labels:\n",
    "            new_labels.append(convert_dict_inverse[label])\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = translate_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(words)\n",
    "vector_size = 50\n",
    "batch_size = 512\n",
    "embedding_size = 64\n",
    "hidden_size = 64\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64000"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size*embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "from tensorflow.keras.backend import set_session\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "config_proto = tf.ConfigProto()\n",
    "off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "config_proto.graph_options.rewrite_options.arithmetic_optimization = off\n",
    "session = tf.Session(config=config_proto)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "sentence_1 = Sequential()\n",
    "sentence_1.add(Embedding(vocabulary_size,embedding_size))\n",
    "\n",
    "               \n",
    "sentence_2 = Sequential()\n",
    "sentence_2.add(Embedding(vocabulary_size,embedding_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_concat = concatenate([sentence_1.output, sentence_2.output])\n",
    "model_concat = Bidirectional(tf.keras.layers.LSTM(hidden_size))(model_concat)\n",
    "model_concat = Dense(hidden_size, activation='relu')(model_concat)\n",
    "model_concat = Dense(3, activation='softmax')(model_concat)\n",
    "model = Model(inputs=[sentence_1.input, sentence_2.input], outputs=model_concat)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_input (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1_input (InputLayer)  [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 64)     64000       embedding_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 64)     64000       embedding_1_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 128)    0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128)          98816       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           8256        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            195         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 235,267\n",
      "Trainable params: 235,267\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353395 samples, validate on 39267 samples\n",
      "Epoch 1/10\n",
      "353395/353395 [==============================] - 714s 2ms/sample - loss: 0.9988 - acc: 0.4858 - val_loss: 0.9772 - val_acc: 0.4983\n",
      "Epoch 2/10\n",
      "   512/353395 [..............................] - ETA: 11:20 - loss: 0.9697 - acc: 0.5156"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "           [X[0],X[1]],num_labels, validation_split=0.1, batch_size=batch_size, epochs=epochs,verbose=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = test_df[\"sentence_1\"].values\n",
    "test_sentence_2 = test_df[\"sentence_2\"].values\n",
    "\n",
    "sentence_1 =tokenizer.texts_to_sequences(train_df[\"sentence_1\"])\n",
    "sentence_2 = tokenizer.texts_to_sequences(train_df[\"sentence_2\"])\n",
    "sentence_1_seq = sequence.pad_sequences(sentence_1, maxlen=max_sequence_length, value=0,truncating=\"post\",padding=\"post\")\n",
    "sentence_2_seq  = sequence.pad_sequences( sentence_2,maxlen=max_sequence_length, value=0,truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
